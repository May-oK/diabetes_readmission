{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027cff67-97b3-48e9-96d5-31cfea2d434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c080315-4857-49fd-bbe0-6d0d34b060e1",
   "metadata": {},
   "source": [
    "## OCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51196870-0b35-44e4-b186-fa2eef555122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b8ba52-49f1-4842-8e36-98f85abe1ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6244879380974055\n",
      "Precision: 0.6250352808354502\n",
      "Recall: 0.4815700771990867\n",
      "F1 Score: 0.544002947859731\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(data['Actual'], data['Predicted'])\n",
    "precision = precision_score(data['Actual'], data['Predicted'])\n",
    "recall = recall_score(data['Actual'], data['Predicted'])\n",
    "f1 = f1_score(data['Actual'], data['Predicted'])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577b631f-7ef6-471c-b5e9-e4fb46af8217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.4815700771990867\n",
      "Specificity: 0.7487708018154312\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(data['Actual'], data['Predicted']).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)  \n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785000bf-6bea-471e-8c6f-adb9d478bc12",
   "metadata": {},
   "source": [
    "## OCT (Sub-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b220e8-4fb1-4f9e-95d6-6d6d99d95f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"predictions_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ef5971-7836-44bc-a7de-8e93488d594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6237799018864107\n",
      "Precision: 0.6223552338530067\n",
      "Recall: 0.48613678373382624\n",
      "F1 Score: 0.5458763201269764\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(data['Actual'], data['Predicted'])\n",
    "precision = precision_score(data['Actual'], data['Predicted'])\n",
    "recall = recall_score(data['Actual'], data['Predicted'])\n",
    "f1 = f1_score(data['Actual'], data['Predicted'])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59be2b02-45ca-4c8b-bda0-513220f91ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.48613678373382624\n",
      "Specificity: 0.7434757942511346\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(data['Actual'], data['Predicted']).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)  \n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bb3c8-3a58-435b-8267-a874a80929d8",
   "metadata": {},
   "source": [
    "## Sparse Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f362683-22ac-45f0-a180-cd6d4cc4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"predictions_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d16f35-d1c1-416b-9347-c9fb1c1b1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6256511404440398\n",
      "Precision: 0.6435311050695666\n",
      "Recall: 0.43753397847124065\n",
      "F1 Score: 0.5209061488673139\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(data['Actual'], data['Predicted'])\n",
    "precision = precision_score(data['Actual'], data['Predicted'])\n",
    "recall = recall_score(data['Actual'], data['Predicted'])\n",
    "f1 = f1_score(data['Actual'], data['Predicted'])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d216766-b601-4ace-8486-ab4a6f338f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.43753397847124065\n",
      "Specificity: 0.7892397881996974\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(data['Actual'], data['Predicted']).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)  \n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4cc38-7c14-4892-969a-7e50f353c805",
   "metadata": {},
   "source": [
    "## Sparse Logistic Regression (Key Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82559659-335d-47f5-b64b-b7e6c74c0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"predictions_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d27986-7782-4b6d-8541-2713c125dc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6226166995397765\n",
      "Precision: 0.6388666559948776\n",
      "Recall: 0.43394585190823093\n",
      "F1 Score: 0.5168350168350168\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(data['Actual'], data['Predicted'])\n",
    "precision = precision_score(data['Actual'], data['Predicted'])\n",
    "recall = recall_score(data['Actual'], data['Predicted'])\n",
    "f1 = f1_score(data['Actual'], data['Predicted'])\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424dcca0-5b71-4e3b-ab05-44dcafb2bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.43394585190823093\n",
      "Specificity: 0.7866868381240545\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(data['Actual'], data['Predicted']).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)  \n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccdd4e-fd07-4216-a36b-e00514564b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
